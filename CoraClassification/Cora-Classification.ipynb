{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from torch_geometric import datasets\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from hydra import compose, initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset': {'name': 'Cora', 'save_path': '.'}, 'gcn_model': {'latent_dim': 256}, 'training': {'train_size_ratio': 0.8, 'epochs': 1000, 'verbose': 10}}\n"
     ]
    }
   ],
   "source": [
    "with initialize(version_base=None, config_path=\"./\"):\n",
    "    config = compose(config_name=\"model.yaml\")\n",
    "    print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora = datasets.Planetoid(root=config.dataset.save_path, name=config.dataset.name, transform=NormalizeFeatures())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number sample: 2708\n",
      "Number of features: 1433\n",
      "Number of classes: 7\n",
      "Number of nodes: 2708\n",
      "Number of training nodes: 140\n",
      "Is undirected? True\n"
     ]
    }
   ],
   "source": [
    "dataset = cora[0]\n",
    "print(f\"Number sample: {len(cora.x)}\")\n",
    "print(f\"Number of features: {cora.num_features}\")\n",
    "print(f\"Number of classes: {cora.num_classes}\")\n",
    "print(f\"Number of nodes: {dataset.num_nodes}\")\n",
    "print(f\"Number of training nodes: {dataset.train_mask.sum()}\")\n",
    "print(f\"Is undirected? {dataset.is_undirected()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = dataset.edge_index.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,  633],\n",
       "        [   0, 1862],\n",
       "        [   0, 2582],\n",
       "        ...,\n",
       "        [2707,  598],\n",
       "        [2707, 1473],\n",
       "        [2707, 2706]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = dataset.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_edges_from(edges.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = {}\n",
    "colors = [\"blue\", \"green\", \"red\", \"yellow\", \"black\", \"orange\", \"magma\"]\n",
    "c_map = {i : colors[i] for i in range(7)}\n",
    "nodes_color = []\n",
    "nodes = []\n",
    "for node in G:\n",
    "  nodes.append(node)\n",
    "  nodes_color.append(c_map[labels[node].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       6\n",
       "1       6\n",
       "2       6\n",
       "3       6\n",
       "4       0\n",
       "       ..\n",
       "2703    5\n",
       "2704    0\n",
       "2705    0\n",
       "2706    0\n",
       "2707    0\n",
       "Length: 2708, dtype: int8"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carac = pd.DataFrame({\"node\" : nodes, \"colors\" : nodes_color})\n",
    "\n",
    "carac['colors']=pd.Categorical(carac['colors'])\n",
    "carac['colors'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./cora_graph.png\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(\"./cora_graph.png\"):\n",
    "  plt.figure(figsize=(100, 50))\n",
    "  pos = nx.kamada_kawai_layout(G)\n",
    "  nx.draw(G, pos, node_color=carac['colors'].cat.codes, with_labels=True)\n",
    "  plt.savefig(\"cora_graph.png\", dpi=150, bbox_inches=\"tight\")\n",
    "  plt.show()\n",
    "else:\n",
    "  print(\"./cora_graph.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LIMIT = int(config.training.train_size_ratio * len(cora.x))\n",
    "TEST_LIMIT = int((1 - config.training.train_size_ratio) * len(cora.x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size) -> None:\n",
    "        \"\"\"The module is a GNN model which is based on the GCN layer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size : The number of features of one node in the graph\n",
    "        hidden_size : The latent dim for hidden layer\n",
    "        output_size : the number of classes which is needed to classify\n",
    "        \"\"\"\n",
    "        super(GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_size, hidden_size)\n",
    "        self.conv2 = GCNConv(hidden_size, hidden_size)\n",
    "        self.conv3 = GCNConv(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, X, edges_index):\n",
    "        out = X\n",
    "        out = self.conv1(out, edges_index)\n",
    "        out = out.relu()\n",
    "        out = F.dropout(out, p=0.5, training=self.training)\n",
    "\n",
    "        out = self.conv2(out, edges_index)\n",
    "        out = out.relu()\n",
    "        out = F.dropout(out, p=0.5, training=self.training)\n",
    "\n",
    "        out = self.conv3(out, edges_index)\n",
    "        out = out.relu()\n",
    "        out = F.dropout(out, p=0.5, training=self.training)\n",
    "\n",
    "        out = self.out(out)\n",
    "        out = F.softmax(out, dim=1)\n",
    "        return out\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return sum(parameter.numel() for parameter in self.parameters())\n",
    "\n",
    "    def __str__(self):\n",
    "        return (\n",
    "            super(GNN, self).__str__()\n",
    "            + f\"\\nNumber of parameters: {self.get_parameters():,}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIM = config.gcn_model.latent_dim\n",
    "model = GNN(cora.num_features, LATENT_DIM, cora.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): GCNConv()\n",
      "  (conv2): GCNConv()\n",
      "  (conv3): GCNConv()\n",
      "  (out): Linear(in_features=256, out_features=7, bias=True)\n",
      ")\n",
      "Number of parameters: 1,799\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = config.training.epochs\n",
    "VERBOSE = config.training.verbose\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "dataset = dataset.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 1.9458249807357788\n",
      "Epoch 2 | Loss: 1.9459444284439087\n",
      "Epoch 3 | Loss: 1.9459552764892578\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Experiments\\CoraClassification\\Cora-Classification.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Experiments/CoraClassification/Cora-Classification.ipynb#X24sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Experiments/CoraClassification/Cora-Classification.ipynb#X24sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Experiments/CoraClassification/Cora-Classification.ipynb#X24sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m out \u001b[39m=\u001b[39m model(dataset\u001b[39m.\u001b[39;49mx, dataset\u001b[39m.\u001b[39;49medge_index)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiments/CoraClassification/Cora-Classification.ipynb#X24sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(out[dataset\u001b[39m.\u001b[39mtrain_mask], dataset\u001b[39m.\u001b[39my[dataset\u001b[39m.\u001b[39mtrain_mask])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiments/CoraClassification/Cora-Classification.ipynb#X24sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\Experiments\\CoraClassification\\Cora-Classification.ipynb Cell 22\u001b[0m in \u001b[0;36mGNN.forward\u001b[1;34m(self, X, edges_index)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiments/CoraClassification/Cora-Classification.ipynb#X24sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mrelu()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiments/CoraClassification/Cora-Classification.ipynb#X24sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(out, p\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Experiments/CoraClassification/Cora-Classification.ipynb#X24sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv3(out, edges_index)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiments/CoraClassification/Cora-Classification.ipynb#X24sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mrelu()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiments/CoraClassification/Cora-Classification.ipynb#X24sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(out, p\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32md:\\Experiments\\CoraClassification\\Cora-Classification.ipynb Cell 22\u001b[0m in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, X, edge_list)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiments/CoraClassification/Cora-Classification.ipynb#X24sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m A_hat \u001b[39m=\u001b[39m A \u001b[39m+\u001b[39m I\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiments/CoraClassification/Cora-Classification.ipynb#X24sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m z \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39minverse(D) \u001b[39m@\u001b[39m A_hat \u001b[39m@\u001b[39m X \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Experiments/CoraClassification/Cora-Classification.ipynb#X24sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m relu \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mReLU()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Experiments/CoraClassification/Cora-Classification.ipynb#X24sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mreturn\u001b[39;00m relu(z)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\activation.py:98\u001b[0m, in \u001b[0;36mReLU.__init__\u001b[1;34m(self, inplace)\u001b[0m\n\u001b[0;32m     95\u001b[0m __constants__ \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39minplace\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     96\u001b[0m inplace: \u001b[39mbool\u001b[39m\n\u001b[1;32m---> 98\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, inplace: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     99\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m    100\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minplace \u001b[39m=\u001b[39m inplace\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = {\n",
    "  \"epoch\" : [],\n",
    "  \"loss\" : []\n",
    "}\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "  model.train()\n",
    "  optimizer.zero_grad()\n",
    "  out = model(dataset.x, dataset.edge_index)\n",
    "  loss = criterion(out[dataset.train_mask], dataset.y[dataset.train_mask])\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "  history[\"epoch\"].append(epoch)\n",
    "  history[\"loss\"].append(loss.cpu().detach().item())\n",
    "  \n",
    "  print(f\"Epoch {epoch} | Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.plot(history[\"epoch\"], history[\"loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "out = model(dataset.x, dataset.edge_index)\n",
    "pred = out.argmax(dim = 1)\n",
    "test_correct = pred[dataset.test_mask] == dataset.y[dataset.test_mask]\n",
    "accuracy = test_correct.sum() / dataset.test_mask.sum()\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "sample_index = random.randint(0, dataset.test_mask.sum()) \n",
    "sample = dataset.x[dataset.test_mask]\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=(20, 10))\n",
    "pred = model(dataset.x, dataset.edge_index)\n",
    "ax = sns.barplot(x=np.array(range(7)), y=pred[sample_index].detach().cpu().numpy())\n",
    "ax.set_xlabel(\"Classes\")\n",
    "ax.set_ylabel(\"Probability\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
